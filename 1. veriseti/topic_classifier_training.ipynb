{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51d1d424",
   "metadata": {},
   "source": [
    "# Topic Classifier Training Pipeline\n",
    "\n",
    "Bu notebook, metin sınıflandırma için hibrit özellik vektörizasyonu kullanan makine öğrenmesi modellerini eğitir.\n",
    "\n",
    "## Pipeline Workflow:\n",
    "1. **Veri Yükleme** - Train/test verilerini yükleme ve ön işleme\n",
    "2. **GloVe Embeddings** - Önceden eğitilmiş word embeddings yükleme  \n",
    "3. **TF-IDF Hesaplaması** - Kategori-bazlı TF-IDF skorları hesaplama\n",
    "4. **Feature Vektörizasyonu** - Hibrit vektör oluşturma (GloVe + TF-IDF weighted)\n",
    "5. **Model Eğitimi** - LogReg (tam vektör) → PCA → Diğer modeller (PCA)\n",
    "6. **Sonuç Analizi** - Performans karşılaştırması\n",
    "\n",
    "## Özellikler:\n",
    "- **Hibrit Vektörler**: GloVe + TF-IDF ağırlıklı kategori vektörleri\n",
    "- **Eksik Kelime İşleme**: Kategorsel TF-IDF tabanlı KNN ile vektör oluşturma\n",
    "- **Hızlı Mod**: Önceden optimize edilmiş parametreler\n",
    "- **PCA Optimizasyonu**: Boyut indirgeme ile hız artışı"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79dc53b",
   "metadata": {},
   "source": [
    "## 1. Kütüphane İmportları ve Konfigürasyon\n",
    "\n",
    "### 1.1 Gerekli Kütüphanelerin İmportları"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f36193b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kütüphaneler başarıyla yüklendi!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import re\n",
    "import warnings\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Tuple\n",
    "from dataclasses import dataclass\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import cross_val_score, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"Kütüphaneler başarıyla yüklendi!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ab6f28",
   "metadata": {},
   "source": [
    "### 1.2 Konfigürasyon Sınıfı\n",
    "\n",
    "Bu hücre training pipeline'ının tüm konfigürasyon ayarlarını tanımlar. Hızlı mod (predefined params) ve hibrit vektör kullanımı gibi seçenekleri içerir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62791517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFIGURATION\n",
      "├── Training Strategy: FAST (predefined params)\n",
      "├── Vector Type: Hybrid (1200D)\n",
      "├── PCA Components: 100\n",
      "├── CV Folds: 3\n",
      "├── KNN Neighbors: 5\n",
      "└── Cache Directory: cache\n",
      "Hybrid Vector Details:\n",
      "   Base GloVe: 300D\n",
      "   Categories: 4 ([1, 2, 3, 4])\n",
      "   Final: 1200D (TF-IDF weighted)\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Training konfigürasyonu\"\"\"\n",
    "    # Data paths\n",
    "    train_path: str = \"archive/train.csv\"\n",
    "    test_path: str = \"archive/test.csv\"\n",
    "    glove_dir: str = \"glove\"\n",
    "    models_dir: str = \"models\"\n",
    "    cache_dir: str = \"cache\"\n",
    "    \n",
    "    # GloVe settings\n",
    "    glove_dim: int = 300\n",
    "    \n",
    "    # TF-IDF settings\n",
    "    tfidf_max_features: int = 5000\n",
    "    \n",
    "    # KNN settings for missing words\n",
    "    knn_neighbors: int = 5\n",
    "    \n",
    "    # PCA settings\n",
    "    pca_components: int = 100\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_folds: int = 3\n",
    "    \n",
    "    # Random search\n",
    "    random_search_iter: int = 20\n",
    "    random_state: int = 42\n",
    "    \n",
    "    # Strategy settings\n",
    "    use_predefined_params: bool = True  # Fast mode with pre-optimized params\n",
    "    use_hybrid_vectors: bool = True     # Category-weighted hybrid vectors\n",
    "    \n",
    "    # Categories\n",
    "    categories: List[int] = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.categories is None:\n",
    "            self.categories = [1, 2, 3, 4]  # World, Sports, Business, Sci/Tech\n",
    "        \n",
    "        # Create directories\n",
    "        os.makedirs(self.models_dir, exist_ok=True)\n",
    "        os.makedirs(self.cache_dir, exist_ok=True)\n",
    "    \n",
    "    @property\n",
    "    def hybrid_vector_dim(self) -> int:\n",
    "        \"\"\"Calculate hybrid vector dimension\"\"\"\n",
    "        if self.use_hybrid_vectors:\n",
    "            return self.glove_dim * len(self.categories)  # 300 * 4 = 1200D\n",
    "        else:\n",
    "            return self.glove_dim\n",
    "\n",
    "# Initialize configuration\n",
    "config = Config()\n",
    "\n",
    "print(\"CONFIGURATION\")\n",
    "print(f\"├── Training Strategy: {'FAST (predefined params)' if config.use_predefined_params else 'SEARCH (RandomizedSearchCV)'}\")\n",
    "print(f\"├── Vector Type: {'Hybrid' if config.use_hybrid_vectors else 'Standard'} ({config.hybrid_vector_dim}D)\")\n",
    "print(f\"├── PCA Components: {config.pca_components}\")\n",
    "print(f\"├── CV Folds: {config.cv_folds}\")\n",
    "print(f\"├── KNN Neighbors: {config.knn_neighbors}\")\n",
    "print(f\"└── Cache Directory: {config.cache_dir}\")\n",
    "\n",
    "if config.use_hybrid_vectors:\n",
    "    print(f\"Hybrid Vector Details:\")\n",
    "    print(f\"   Base GloVe: {config.glove_dim}D\")\n",
    "    print(f\"   Categories: {len(config.categories)} ({config.categories})\")\n",
    "    print(f\"   Final: {config.hybrid_vector_dim}D (TF-IDF weighted)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8438e6",
   "metadata": {},
   "source": [
    "## 2. Utility Classes (Yardımcı Sınıflar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d44c5a",
   "metadata": {},
   "source": [
    "### 2.1 Metin Temizleme Sınıfı\n",
    "\n",
    "Bu hücre metinleri temizlemek için kullanılan TextPreprocessor sınıfını tanımlar. URL'ler, email'ler, sayılar ve özel karakterleri kaldırır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bbc03fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextPreprocessor sınıfı tanımlandı\n"
     ]
    }
   ],
   "source": [
    "class TextPreprocessor:\n",
    "    \"\"\"Text preprocessing utilities\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def clean_text(text: str) -> str:\n",
    "        \"\"\"Advanced text cleaning\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        text = text.lower()\n",
    "        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "        text = re.sub(r'\\S+@\\S+', '', text)\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        text = re.sub(r'\\b\\w{1,2}\\b', '', text)\n",
    "        text = re.sub(r'\\b\\w{15,}\\b', '', text)\n",
    "        text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "\n",
    "print(\"TextPreprocessor sınıfı tanımlandı\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a784473a",
   "metadata": {},
   "source": [
    "### 2.2 GloVe Embeddings Yükleme Sınıfı\n",
    "\n",
    "Bu hücre GloVe word embeddings'lerini yüklemek için kullanılan sınıfı tanımlar. Otomatik download ve progress bar özellikleri içerir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7b3e04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVeLoader sınıfı tanımlandı\n"
     ]
    }
   ],
   "source": [
    "class GloVeLoader:\n",
    "    \"\"\"GloVe embeddings loader with progress tracking\"\"\"\n",
    "    \n",
    "    def __init__(self, glove_dir: str, dim: int = 300):\n",
    "        self.glove_dir = glove_dir\n",
    "        self.dim = dim\n",
    "        self.embeddings_index = {}\n",
    "    \n",
    "    def load_embeddings(self) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Load GloVe vectors (auto-download if needed)\"\"\"\n",
    "        # Auto-download GloVe files if they don't exist\n",
    "        glove_path = self.download_glove_model()\n",
    "        \n",
    "        print(f\"Loading GloVe vectors from: {glove_path}\")\n",
    "        \n",
    "        # Count total lines for progress bar\n",
    "        print(\"Counting lines for progress tracking...\")\n",
    "        total_lines = sum(1 for _ in open(glove_path, encoding='utf-8'))\n",
    "        \n",
    "        with open(glove_path, encoding='utf-8') as f:\n",
    "            for line in tqdm(f, desc=\"Loading GloVe\", total=total_lines, unit=\"words\"):\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                coefs = np.asarray(values[1:], dtype='float32')\n",
    "                self.embeddings_index[word] = coefs\n",
    "        \n",
    "        print(f\"Loaded {len(self.embeddings_index):,} word vectors\")\n",
    "        return self.embeddings_index\n",
    "    \n",
    "    def download_glove_model(self) -> str:\n",
    "        \"\"\"Download GloVe embeddings if not available\"\"\"\n",
    "        glove_path = os.path.join(self.glove_dir, f\"glove.6B.{self.dim}d.txt\")\n",
    "        \n",
    "        if not os.path.exists(glove_path):\n",
    "            print(f\"GloVe vektörleri indiriliyor (boyut: {self.dim}d)...\")\n",
    "            print(\"Not: Bu büyük bir dosya, indirme işlemi birkaç dakika sürebilir.\")\n",
    "            \n",
    "            # Create glove directory if it doesn't exist\n",
    "            os.makedirs(self.glove_dir, exist_ok=True)\n",
    "            \n",
    "            try:\n",
    "                url = \"https://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "                print(\"İndirme başlıyor...\")\n",
    "                r = requests.get(url, stream=True, timeout=120)\n",
    "                if r.status_code == 200:\n",
    "                    print(\"İndirme tamamlandı, zipten çıkarılıyor...\")\n",
    "                    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "                    z.extractall(self.glove_dir)\n",
    "                    print(\"GloVe vektörleri başarıyla indirildi ve çıkarıldı.\")\n",
    "                else:\n",
    "                    print(f\"İndirme hatası: {r.status_code}\")\n",
    "                    raise Exception(\"GloVe vektörleri indirilemedi.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Hata: {e}\")\n",
    "                print(\"GloVe vektörlerini manuel olarak indirmeniz gerekebilir.\")\n",
    "                print(\"İndirme linki: https://nlp.stanford.edu/data/glove.6B.zip\")\n",
    "                print(f\"Dosyayı {self.glove_dir} klasörüne çıkarın.\")\n",
    "                raise Exception(\"GloVe vektörleri yüklenemedi.\")\n",
    "        else:\n",
    "            print(f\"GloVe vektörleri zaten mevcut: {glove_path}\")\n",
    "        \n",
    "        return glove_path\n",
    "\n",
    "print(\"GloVeLoader sınıfı tanımlandı\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054380ff",
   "metadata": {},
   "source": [
    "### 2.3 TF-IDF Skor Hesaplama Sınıfı\n",
    "\n",
    "Bu hücre cache kullanmadan TF-IDF skorlarını her seferinde yeniden hesaplayan SimpleTFIDFBuilder sınıfını tanımlar. Kategori-bazlı TF-IDF hesaplamaları yapar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6256419f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleTFIDFBuilder sınıfı tanımlandı\n"
     ]
    }
   ],
   "source": [
    "class SimpleTFIDFBuilder:\n",
    "    \"\"\"Simplified TF-IDF builder without caching - calculates everything from scratch\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.word_score_cache = {}\n",
    "        self.category_vectorizers = {}\n",
    "    \n",
    "    def build_tfidf_scores(self, texts: List[str], labels: List[int]) -> Dict:\n",
    "        \"\"\"Build TF-IDF scores from scratch without any caching\"\"\"\n",
    "        print(\"Building TF-IDF scores from scratch...\")\n",
    "        \n",
    "        # Clean texts\n",
    "        print(\"Cleaning texts...\")\n",
    "        cleaned_texts = [TextPreprocessor.clean_text(text) for text in tqdm(texts, desc=\"Cleaning texts\")]\n",
    "        \n",
    "        # Build category-specific TF-IDF vectorizers\n",
    "        print(\"Building category-specific TF-IDF vectorizers...\")\n",
    "        for category in tqdm(self.config.categories, desc=\"Processing categories\"):\n",
    "            print(f\"Processing category {category}...\")\n",
    "            \n",
    "            # Filter texts for this category\n",
    "            category_texts = [\n",
    "                cleaned_texts[i] for i, label in enumerate(labels) \n",
    "                if label == category\n",
    "            ]\n",
    "            \n",
    "            if not category_texts:\n",
    "                print(f\"Warning: No texts found for category {category}!\")\n",
    "                continue\n",
    "            \n",
    "            # Create TF-IDF vectorizer for this category\n",
    "            vectorizer = TfidfVectorizer(\n",
    "                max_features=self.config.tfidf_max_features,\n",
    "                stop_words='english',\n",
    "                ngram_range=(1, 1),\n",
    "                lowercase=True,\n",
    "                token_pattern=r'\\b[a-zA-Z]{3,}\\b'\n",
    "            )\n",
    "            \n",
    "            # Fit and store vectorizer\n",
    "            vectorizer.fit(category_texts)\n",
    "            self.category_vectorizers[category] = vectorizer\n",
    "        \n",
    "        # Build word score cache\n",
    "        self._build_word_score_cache(cleaned_texts, labels)\n",
    "        \n",
    "        print(f\"TF-IDF scores built for {len(self.word_score_cache):,} words\")\n",
    "        return self.word_score_cache\n",
    "    \n",
    "    def _build_word_score_cache(self, cleaned_texts: List[str], labels: List[int]):\n",
    "        \"\"\"Build optimized word score cache\"\"\"\n",
    "        print(\"Building optimized word score cache...\")\n",
    "        \n",
    "        # Pre-compute category data\n",
    "        print(\"Pre-computing TF-IDF matrices...\")\n",
    "        category_data = {}\n",
    "        \n",
    "        for category in tqdm(self.config.categories, desc=\"Pre-computing matrices\"):\n",
    "            if category in self.category_vectorizers:\n",
    "                vectorizer = self.category_vectorizers[category]\n",
    "                \n",
    "                # Filter category texts\n",
    "                category_texts = [\n",
    "                    cleaned_texts[i] for i, label in enumerate(labels) \n",
    "                    if label == category\n",
    "                ]\n",
    "                \n",
    "                if category_texts:\n",
    "                    # Transform texts to TF-IDF matrix\n",
    "                    tfidf_matrix = vectorizer.transform(category_texts)\n",
    "                    feature_names = vectorizer.get_feature_names_out()\n",
    "                    \n",
    "                    # Compute mean scores for all features\n",
    "                    feature_scores = np.array(tfidf_matrix.mean(axis=0)).flatten()\n",
    "                    \n",
    "                    # Create fast lookup dictionary\n",
    "                    feature_to_idx = {word: idx for idx, word in enumerate(feature_names)}\n",
    "                    \n",
    "                    category_data[category] = {\n",
    "                        'feature_names': feature_names,\n",
    "                        'feature_scores': feature_scores,\n",
    "                        'feature_to_idx': feature_to_idx\n",
    "                    }\n",
    "                    \n",
    "                    print(f\"Category {category}: {len(feature_names)} features pre-computed\")\n",
    "        \n",
    "        # Collect all unique words\n",
    "        all_words = set()\n",
    "        for data in category_data.values():\n",
    "            all_words.update(data['feature_names'])\n",
    "            \n",
    "        # Add words from texts that might not be in any category features\n",
    "        stop_words = _stop_words.ENGLISH_STOP_WORDS\n",
    "        for text in cleaned_texts:\n",
    "            tokens = [t for t in text.split() if t not in stop_words and len(t) >= 3]\n",
    "            all_words.update(tokens)\n",
    "        \n",
    "        print(f\"Found {len(all_words):,} unique words total\")\n",
    "        \n",
    "        # Build cache from pre-computed data\n",
    "        print(\"Building cache from pre-computed data...\")\n",
    "        for word in tqdm(all_words, desc=\"Building word cache\"):\n",
    "            self.word_score_cache[word] = {}\n",
    "            \n",
    "            for category in self.config.categories:\n",
    "                if (category in category_data and \n",
    "                    word in category_data[category]['feature_to_idx']):\n",
    "                    # Fast lookup from pre-computed data\n",
    "                    word_idx = category_data[category]['feature_to_idx'][word]\n",
    "                    score = category_data[category]['feature_scores'][word_idx]\n",
    "                    self.word_score_cache[word][category] = float(score) if not np.isnan(score) else 0.0\n",
    "                else:\n",
    "                    self.word_score_cache[word][category] = 0.0\n",
    "\n",
    "print(\"SimpleTFIDFBuilder sınıfı tanımlandı\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7744b02",
   "metadata": {},
   "source": [
    "### 2.4 Eksik Kelime İşleme Sınıfı\n",
    "\n",
    "Bu hücre GloVe'de bulunmayan kelimeler için TF-IDF bazlı KNN ile vektör oluşturulur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "534f3337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDFBasedKNNHandler sınıfı tanımlandı\n"
     ]
    }
   ],
   "source": [
    "class TFIDFBasedKNNHandler:\n",
    "    \"\"\"TF-IDF based KNN handler for missing words\"\"\"\n",
    "    \n",
    "    def __init__(self, embeddings_index: Dict, word_score_cache: Dict, config: Config):\n",
    "        self.embeddings_index = embeddings_index\n",
    "        self.word_score_cache = word_score_cache\n",
    "        self.config = config\n",
    "    \n",
    "    def find_top_words_by_tfidf(self, target_category: int, min_score: float = 0.01) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Find top words by TF-IDF score for target category\"\"\"\n",
    "        category_words = []\n",
    "        \n",
    "        for word, scores in self.word_score_cache.items():\n",
    "            category_score = scores.get(target_category, 0.0)\n",
    "            if category_score > min_score and word in self.embeddings_index:\n",
    "                category_words.append((word, category_score))\n",
    "        \n",
    "        return sorted(category_words, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    def create_missing_word_vector(self, missing_word: str, target_category: int) -> np.ndarray:\n",
    "        \"\"\"Create vector for missing word using TF-IDF weighted KNN\"\"\"\n",
    "        top_words = self.find_top_words_by_tfidf(target_category)\n",
    "        \n",
    "        if not top_words:\n",
    "            return np.random.normal(0, 0.1, size=self.config.glove_dim)\n",
    "        \n",
    "        top_neighbors = top_words[:self.config.knn_neighbors]\n",
    "        \n",
    "        vectors = []\n",
    "        weights = []\n",
    "        \n",
    "        for word, tfidf_score in top_neighbors:\n",
    "            if word in self.embeddings_index:\n",
    "                vector = self.embeddings_index[word]\n",
    "                weight = tfidf_score\n",
    "                \n",
    "                vectors.append(vector)\n",
    "                weights.append(weight)\n",
    "        \n",
    "        if not vectors:\n",
    "            return np.random.normal(0, 0.1, size=self.config.glove_dim)\n",
    "        \n",
    "        # Weighted average based on TF-IDF scores\n",
    "        vectors = np.array(vectors)\n",
    "        weights = np.array(weights)\n",
    "        weights = weights / weights.sum()  # Normalize\n",
    "        \n",
    "        return np.average(vectors, axis=0, weights=weights)\n",
    "    \n",
    "    def get_category_statistics(self, target_category: int) -> Dict:\n",
    "        \"\"\"Get category statistics for debugging\"\"\"\n",
    "        top_words = self.find_top_words_by_tfidf(target_category)\n",
    "        \n",
    "        if not top_words:\n",
    "            return {'total_words': 0, 'avg_score': 0, 'top_5': []}\n",
    "        \n",
    "        scores = [score for _, score in top_words]\n",
    "        \n",
    "        return {\n",
    "            'total_words': len(top_words),\n",
    "            'avg_score': np.mean(scores),\n",
    "            'max_score': max(scores),\n",
    "            'min_score': min(scores),\n",
    "            'top_5': top_words[:5]\n",
    "        }\n",
    "\n",
    "print(\"TFIDFBasedKNNHandler sınıfı tanımlandı\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd44e156",
   "metadata": {},
   "source": [
    "### 2.5 Feature Vektörizasyon Sınıfı\n",
    "\n",
    "Bu hücre hibrit feature vektörleri oluşturan FeatureVectorizer sınıfını tanımlar. GloVe + TF-IDF ağırlıklı kategori vektörleri oluşturur ve PCA transformer'ını içerir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4262d5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeatureVectorizer sınıfı tanımlandı\n"
     ]
    }
   ],
   "source": [
    "class FeatureVectorizer:\n",
    "    \"\"\"Main feature vectorization class\"\"\"\n",
    "    \n",
    "    def __init__(self, embeddings_index: Dict, word_score_cache: Dict, config: Config):\n",
    "        self.embeddings_index = embeddings_index.copy()\n",
    "        self.word_score_cache = word_score_cache\n",
    "        self.config = config\n",
    "        self.missing_word_handler = TFIDFBasedKNNHandler(embeddings_index, word_score_cache, config)\n",
    "        self.missing_word_cache = {}\n",
    "        self.pca = PCA(n_components=config.pca_components, random_state=config.random_state)\n",
    "        self.is_fitted = False\n",
    "    \n",
    "    def text_to_vector(self, text: str, target_category: int) -> np.ndarray:\n",
    "        \"\"\"Convert single text to vector\"\"\"\n",
    "        cleaned_text = TextPreprocessor.clean_text(text)\n",
    "        tokens = cleaned_text.split()\n",
    "        stop_words = _stop_words.ENGLISH_STOP_WORDS\n",
    "        filtered_tokens = [t for t in tokens if t not in stop_words and len(t) >= 3]\n",
    "        \n",
    "        if not filtered_tokens:\n",
    "            if self.config.use_hybrid_vectors:\n",
    "                return np.zeros(self.config.hybrid_vector_dim)\n",
    "            else:\n",
    "                return np.zeros(self.config.glove_dim)\n",
    "        \n",
    "        weighted_vectors = []\n",
    "        total_weight = 0\n",
    "        \n",
    "        for token in filtered_tokens:\n",
    "            if token in self.embeddings_index:\n",
    "                # Word exists in GloVe\n",
    "                base_vector = self.embeddings_index[token]\n",
    "                weight = self._calculate_word_weight(token, target_category)\n",
    "                \n",
    "                if self.config.use_hybrid_vectors:\n",
    "                    hybrid_vector = self._create_hybrid_vector(base_vector, token)\n",
    "                    weighted_vectors.append(hybrid_vector * weight)\n",
    "                else:\n",
    "                    weighted_vectors.append(base_vector * weight)\n",
    "                \n",
    "                total_weight += weight\n",
    "                \n",
    "            else:\n",
    "                # Missing word - handle with KNN\n",
    "                if token in self.missing_word_cache:\n",
    "                    base_vector = self.missing_word_cache[token]\n",
    "                else:\n",
    "                    base_vector = self.missing_word_handler.create_missing_word_vector(token, target_category)\n",
    "                    self.missing_word_cache[token] = base_vector\n",
    "                    self.embeddings_index[token] = base_vector\n",
    "                \n",
    "                if self.config.use_hybrid_vectors:\n",
    "                    hybrid_vector = self._create_hybrid_vector(base_vector, token)\n",
    "                    weighted_vectors.append(hybrid_vector * 0.2)  # Lower weight for missing words\n",
    "                else:\n",
    "                    weighted_vectors.append(base_vector * 0.2)\n",
    "                \n",
    "                total_weight += 0.2\n",
    "        \n",
    "        if not weighted_vectors or total_weight == 0:\n",
    "            if self.config.use_hybrid_vectors:\n",
    "                return np.zeros(self.config.hybrid_vector_dim)\n",
    "            else:\n",
    "                return np.zeros(self.config.glove_dim)\n",
    "        \n",
    "        # Weighted average\n",
    "        return np.sum(weighted_vectors, axis=0) / total_weight\n",
    "    \n",
    "    def _create_hybrid_vector(self, base_vector: np.ndarray, word: str) -> np.ndarray:\n",
    "        \"\"\"Create hybrid vector: 300D GloVe → 1200D (4×TF-IDF weighted)\"\"\"\n",
    "        hybrid_parts = []\n",
    "        \n",
    "        # For each category, create TF-IDF weighted vector\n",
    "        for category in self.config.categories:\n",
    "            tfidf_score = self.word_score_cache.get(word, {}).get(category, 0.0)\n",
    "            category_weighted_vector = base_vector * tfidf_score\n",
    "            hybrid_parts.append(category_weighted_vector)\n",
    "        \n",
    "        # Concatenate: 300D + 300D + 300D + 300D = 1200D\n",
    "        return np.concatenate(hybrid_parts)\n",
    "    \n",
    "    def _calculate_word_weight(self, word: str, target_category: int) -> float:\n",
    "        \"\"\"Calculate word weight based on TF-IDF scores\"\"\"\n",
    "        if word not in self.word_score_cache:\n",
    "            return 0.1\n",
    "        \n",
    "        category_scores = [\n",
    "            self.word_score_cache[word].get(cat, 0.0) \n",
    "            for cat in self.config.categories\n",
    "        ]\n",
    "        \n",
    "        target_score = category_scores[target_category - 1]  # 0-indexed\n",
    "        \n",
    "        if target_score <= 0:\n",
    "            return 0.1\n",
    "        \n",
    "        # Cross-category discriminativeness\n",
    "        other_scores = [\n",
    "            category_scores[i] for i in range(len(self.config.categories))\n",
    "            if i != (target_category - 1)\n",
    "        ]\n",
    "        \n",
    "        mean_other = np.mean(other_scores) if other_scores else 0\n",
    "        std_other = np.std(other_scores) if len(other_scores) > 1 else 0.01\n",
    "        \n",
    "        # Z-score based distinctiveness\n",
    "        if std_other > 0:\n",
    "            z_score = (target_score - mean_other) / std_other\n",
    "            distinctiveness = max(0, min(2, z_score)) / 2.0\n",
    "        else:\n",
    "            distinctiveness = min(target_score / (mean_other + 0.01), 2.0) / 2.0\n",
    "        \n",
    "        final_weight = target_score * (1.0 + distinctiveness)\n",
    "        return min(final_weight, 2.0)\n",
    "    \n",
    "    def texts_to_vectors(self, texts: List[str], labels: List[int]) -> np.ndarray:\n",
    "        \"\"\"Convert texts to feature vectors\"\"\"\n",
    "        print(\"Converting texts to vectors...\")\n",
    "        \n",
    "        vectors = []\n",
    "        for text, label in tqdm(zip(texts, labels), total=len(texts), desc=\"Vectorizing texts\"):\n",
    "            vector = self.text_to_vector(text, label)\n",
    "            vectors.append(vector)\n",
    "        \n",
    "        return np.array(vectors)\n",
    "    \n",
    "    def fit_pca(self, vectors: np.ndarray):\n",
    "        \"\"\"Fit PCA transformer\"\"\"\n",
    "        input_dim = vectors.shape[1]\n",
    "        print(f\"PCA fitting: {input_dim}D → {self.config.pca_components}D\")\n",
    "        \n",
    "        if self.config.use_hybrid_vectors:\n",
    "            print(f\"Input: Hybrid vectors ({len(self.config.categories)}×{self.config.glove_dim}D TF-IDF weighted)\")\n",
    "        \n",
    "        self.pca.fit(vectors)\n",
    "        self.is_fitted = True\n",
    "        \n",
    "        explained_var = self.pca.explained_variance_ratio_.sum()\n",
    "        print(f\"PCA fitted, explained variance ratio: {explained_var:.4f}\")\n",
    "    \n",
    "    def transform_pca(self, vectors: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Transform vectors using fitted PCA\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"PCA not fitted yet!\")\n",
    "        return self.pca.transform(vectors)\n",
    "\n",
    "print(\"FeatureVectorizer sınıfı tanımlandı\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890918aa",
   "metadata": {},
   "source": [
    "## 3. Model Classes (Model Sınıfları)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afad4048",
   "metadata": {},
   "source": [
    "### 3.1 Temel Model Sınıfı\n",
    "\n",
    "Bu hücre tüm makine öğrenmesi modellerinin kalıtım alacağı abstract BaseModel sınıfını tanımlar. Hiperparametre optimizasyonu için ortak arayüz sağlar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20b902dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModel sınıfı tanımlandı\n"
     ]
    }
   ],
   "source": [
    "class BaseModel(ABC):\n",
    "    \"\"\"Base model class with hyperparameter optimization\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.model = None\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_model(self):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_param_grid(self) -> Dict:\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_predefined_params(self) -> Dict:\n",
    "        pass\n",
    "    \n",
    "    def hyperparameter_search(self, X: np.ndarray, y: np.ndarray) -> Dict:\n",
    "        \"\"\"Hyperparameter tuning: predefined params or RandomizedSearchCV\"\"\"\n",
    "        \n",
    "        if self.config.use_predefined_params:\n",
    "            # Use predefined optimal parameters (FAST)\n",
    "            print(f\"Using predefined optimal params for {self.__class__.__name__}...\")\n",
    "            search_start = time.time()\n",
    "            \n",
    "            predefined_params = self.get_predefined_params()\n",
    "            if predefined_params:\n",
    "                # Create model with optimal parameters\n",
    "                model = self.get_model()\n",
    "                model.set_params(**predefined_params)\n",
    "                \n",
    "                # Cross validation to measure performance\n",
    "                cv_scores = cross_val_score(\n",
    "                    model, X, y, \n",
    "                    cv=self.config.cv_folds, \n",
    "                    scoring='accuracy',\n",
    "                    n_jobs=-1\n",
    "                )\n",
    "                \n",
    "                # Full fit\n",
    "                model.fit(X, y)\n",
    "                self.model = model\n",
    "                \n",
    "                search_time = time.time() - search_start\n",
    "                print(f\"Predefined params applied in {search_time:.2f}s\")\n",
    "                \n",
    "                return {\n",
    "                    'best_score': cv_scores.mean(),\n",
    "                    'best_params': predefined_params,\n",
    "                    'cv_results': None,\n",
    "                    'best_estimator': model,\n",
    "                    'search_time': search_time,\n",
    "                    'method': 'predefined'\n",
    "                }\n",
    "            else:\n",
    "                print(\"No predefined params, falling back to default training...\")\n",
    "                return self._train_with_cv(X, y)\n",
    "        \n",
    "        else:\n",
    "            # RandomizedSearchCV hyperparameter search (SLOW)\n",
    "            print(f\"Hyperparameter search for {self.__class__.__name__}...\")\n",
    "            search_start = time.time()\n",
    "            \n",
    "            model = self.get_model()\n",
    "            param_grid = self.get_param_grid()\n",
    "            \n",
    "            if not param_grid:\n",
    "                print(\"No param grid, using default training...\")\n",
    "                return self._train_with_cv(X, y)\n",
    "            \n",
    "            search = RandomizedSearchCV(\n",
    "                model,\n",
    "                param_grid,\n",
    "                n_iter=self.config.random_search_iter,\n",
    "                cv=self.config.cv_folds,\n",
    "                scoring='accuracy',\n",
    "                random_state=self.config.random_state,\n",
    "                n_jobs=-1,\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            search.fit(X, y)\n",
    "            self.model = search.best_estimator_\n",
    "            \n",
    "            search_time = time.time() - search_start\n",
    "            print(f\"Search completed in {search_time:.2f}s\")\n",
    "            \n",
    "            return {\n",
    "                'best_score': search.best_score_,\n",
    "                'best_params': search.best_params_,\n",
    "                'cv_results': search.cv_results_,\n",
    "                'best_estimator': search.best_estimator_,\n",
    "                'search_time': search_time,\n",
    "                'method': 'random_search'\n",
    "            }\n",
    "    \n",
    "    def _train_with_cv(self, X: np.ndarray, y: np.ndarray) -> Dict:\n",
    "        \"\"\"Cross validation ile train et\"\"\"\n",
    "        model = self.get_model()\n",
    "        \n",
    "        # Cross validation\n",
    "        cv_scores = cross_val_score(\n",
    "            model, X, y, \n",
    "            cv=self.config.cv_folds, \n",
    "            scoring='accuracy',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        # Full fit\n",
    "        model.fit(X, y)\n",
    "        self.model = model\n",
    "        \n",
    "        return {\n",
    "            'best_score': cv_scores.mean(),\n",
    "            'cv_std': cv_scores.std(),\n",
    "            'cv_scores': cv_scores,\n",
    "            'best_estimator': model,\n",
    "            'method': 'cv_only'\n",
    "        }\n",
    "\n",
    "print(\"BaseModel sınıfı tanımlandı\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d48c5ae",
   "metadata": {},
   "source": [
    "### 3.2 Logistic Regression Model Sınıfı\n",
    "\n",
    "Bu hücre LogisticRegression modeli için özel sınıfı tanımlar. Hiperparametre gridini ve önceden optimize edilmiş parametreleri içerir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "018839ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tüm model sınıfları tanımlandı\n"
     ]
    }
   ],
   "source": [
    "class LogisticRegressionModel(BaseModel):\n",
    "    def get_model(self):\n",
    "        return LogisticRegression(max_iter=2000, random_state=self.config.random_state, n_jobs=-1)\n",
    "    \n",
    "    def get_param_grid(self) -> Dict:\n",
    "        return {\n",
    "            'C': [0.01, 0.1, 1, 10, 100],\n",
    "            'penalty': ['l1', 'l2'],\n",
    "            'solver': ['liblinear', 'saga']\n",
    "        }\n",
    "    \n",
    "    def get_predefined_params(self) -> Dict:\n",
    "        return {\n",
    "            'C': 100,\n",
    "            'penalty': 'l1',\n",
    "            'solver': 'liblinear'\n",
    "        }\n",
    "\n",
    "class SVMModel(BaseModel):\n",
    "    def get_model(self):\n",
    "        return SVC(random_state=self.config.random_state)\n",
    "    \n",
    "    def get_param_grid(self) -> Dict:\n",
    "        return {\n",
    "            'C': [0.1, 1, 10, 100],\n",
    "            'gamma': ['scale', 'auto', 0.001, 0.01, 0.1],\n",
    "            'kernel': ['rbf', 'linear', 'poly']\n",
    "        }\n",
    "    \n",
    "    def get_predefined_params(self) -> Dict:\n",
    "        return {\n",
    "            'C': 100,\n",
    "            'gamma': 'scale',\n",
    "            'kernel': 'rbf'\n",
    "        }\n",
    "\n",
    "class MLPModel(BaseModel):\n",
    "    def get_model(self):\n",
    "        return MLPClassifier(\n",
    "            random_state=self.config.random_state,\n",
    "            max_iter=500\n",
    "        )\n",
    "    \n",
    "    def get_param_grid(self) -> Dict:\n",
    "        return {\n",
    "            'hidden_layer_sizes': [(100,), (200,), (100, 50), (200, 100)],\n",
    "            'activation': ['relu', 'tanh'],\n",
    "            'alpha': [0.0001, 0.001, 0.01],\n",
    "            'learning_rate': ['constant', 'adaptive']\n",
    "        }\n",
    "    \n",
    "    def get_predefined_params(self) -> Dict:\n",
    "        return {\n",
    "            'hidden_layer_sizes': (200, 100),\n",
    "            'activation': 'relu',\n",
    "            'alpha': 0.0001,\n",
    "            'learning_rate': 'constant'\n",
    "        }\n",
    "\n",
    "class GradientBoostingModel(BaseModel):\n",
    "    def get_model(self):\n",
    "        return GradientBoostingClassifier(random_state=self.config.random_state)\n",
    "    \n",
    "    def get_param_grid(self) -> Dict:\n",
    "        return {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'subsample': [0.8, 0.9, 1.0]\n",
    "        }\n",
    "    \n",
    "    def get_predefined_params(self) -> Dict:\n",
    "        return {\n",
    "            'n_estimators': 200,\n",
    "            'learning_rate': 0.2,\n",
    "            'max_depth': 5,\n",
    "            'subsample': 0.9\n",
    "        }\n",
    "\n",
    "print(\"Tüm model sınıfları tanımlandı\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb60688",
   "metadata": {},
   "source": [
    "## 4. Veri Yükleme ve Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9f20ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Data loaded - Train: 120000, Test: 7600\n",
      "Distribution: {1: np.int64(30000), 2: np.int64(30000), 3: np.int64(30000), 4: np.int64(30000)}\n",
      "\n",
      "Sample Training Data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reuters)</td>\n",
       "      <td>Reuters - Short-sellers, Wall Street's dwindli...</td>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "      <td>Reuters - Private investment firm Carlyle Grou...</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters)</td>\n",
       "      <td>Reuters - Soaring crude prices plus worries\\ab...</td>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "      <td>Reuters - Authorities have halted oil export\\f...</td>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "      <td>AFP - Tearaway world oil prices, toppling reco...</td>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                              title  \\\n",
       "1      3  Wall St. Bears Claw Back Into the Black (Reuters)   \n",
       "2      3  Carlyle Looks Toward Commercial Aerospace (Reu...   \n",
       "3      3    Oil and Economy Cloud Stocks' Outlook (Reuters)   \n",
       "4      3  Iraq Halts Oil Exports from Main Southern Pipe...   \n",
       "5      3  Oil prices soar to all-time record, posing new...   \n",
       "\n",
       "                                         description  \\\n",
       "1  Reuters - Short-sellers, Wall Street's dwindli...   \n",
       "2  Reuters - Private investment firm Carlyle Grou...   \n",
       "3  Reuters - Soaring crude prices plus worries\\ab...   \n",
       "4  Reuters - Authorities have halted oil export\\f...   \n",
       "5  AFP - Tearaway world oil prices, toppling reco...   \n",
       "\n",
       "                                                text  \n",
       "1  Wall St. Bears Claw Back Into the Black (Reute...  \n",
       "2  Carlyle Looks Toward Commercial Aerospace (Reu...  \n",
       "3  Oil and Economy Cloud Stocks' Outlook (Reuters...  \n",
       "4  Iraq Halts Oil Exports from Main Southern Pipe...  \n",
       "5  Oil prices soar to all-time record, posing new...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label Distribution:\n",
      "  1 (World): 30,000 samples\n",
      "  2 (Sports): 30,000 samples\n",
      "  3 (Business): 30,000 samples\n",
      "  4 (Sci/Tech): 30,000 samples\n"
     ]
    }
   ],
   "source": [
    "def load_data(config: Config) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Load and preprocess training and test data\"\"\"\n",
    "    print(\"Loading datasets...\")\n",
    "    \n",
    "    # Load training data\n",
    "    train_df = pd.read_csv(config.train_path, header=None)\n",
    "    train_df.columns = [\"label\", \"title\", \"description\"]\n",
    "    train_df = train_df[train_df[\"label\"] != \"Class Index\"]\n",
    "    train_df[\"label\"] = train_df[\"label\"].astype(int)\n",
    "    train_df[\"text\"] = train_df[\"title\"] + \" \" + train_df[\"description\"]\n",
    "    \n",
    "    # Load test data\n",
    "    test_df = pd.read_csv(config.test_path, header=None)\n",
    "    test_df.columns = [\"label\", \"title\", \"description\"]\n",
    "    test_df = test_df[test_df[\"label\"] != \"Class Index\"]\n",
    "    test_df[\"label\"] = test_df[\"label\"].astype(int)\n",
    "    test_df[\"text\"] = test_df[\"title\"] + \" \" + test_df[\"description\"]\n",
    "    \n",
    "    print(f\"Data loaded - Train: {len(train_df)}, Test: {len(test_df)}\")\n",
    "    print(f\"Distribution: {dict(train_df['label'].value_counts().sort_index())}\")\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "# Execute data loading\n",
    "train_df, test_df = load_data(config)\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\nSample Training Data:\")\n",
    "display(train_df.head())\n",
    "\n",
    "print(\"\\nLabel Distribution:\")\n",
    "label_counts = train_df['label'].value_counts().sort_index()\n",
    "label_names = {1: 'World', 2: 'Sports', 3: 'Business', 4: 'Sci/Tech'}\n",
    "for label, count in label_counts.items():\n",
    "    print(f\"  {label} ({label_names[label]}): {count:,} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320b90df",
   "metadata": {},
   "source": [
    "## 5. GloVe Embeddings ve TF-IDF Skorları"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08592834",
   "metadata": {},
   "source": [
    "### 5.1 Embeddings ve TF-IDF Kurulum Fonksiyonu\n",
    "\n",
    "Bu hücre GloVe embeddings'lerini yükleyen ve TF-IDF skorlarını hesaplayan ana kurulum fonksiyonunu tanımlar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9200ce7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe embeddings...\n",
      "GloVe vektörleri zaten mevcut: glove\\glove.6B.300d.txt\n",
      "Loading GloVe vectors from: glove\\glove.6B.300d.txt\n",
      "Counting lines for progress tracking...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading GloVe: 100%|██████████| 400000/400000 [00:19<00:00, 20490.09words/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400,000 word vectors\n",
      "Building TF-IDF scores...\n",
      "Building TF-IDF scores from scratch...\n",
      "Cleaning texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning texts: 100%|██████████| 120000/120000 [00:04<00:00, 26619.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building category-specific TF-IDF vectorizers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing categories:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing category 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing categories:  25%|██▌       | 1/4 [00:00<00:01,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing category 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing categories:  50%|█████     | 2/4 [00:01<00:00,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing category 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing categories:  75%|███████▌  | 3/4 [00:01<00:00,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing category 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing categories: 100%|██████████| 4/4 [00:01<00:00,  2.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building optimized word score cache...\n",
      "Pre-computing TF-IDF matrices...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pre-computing matrices:  25%|██▌       | 1/4 [00:00<00:01,  2.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category 1: 5000 features pre-computed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pre-computing matrices:  50%|█████     | 2/4 [00:00<00:00,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category 2: 5000 features pre-computed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pre-computing matrices:  75%|███████▌  | 3/4 [00:01<00:00,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category 3: 5000 features pre-computed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pre-computing matrices: 100%|██████████| 4/4 [00:01<00:00,  2.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category 4: 5000 features pre-computed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 83,918 unique words total\n",
      "Building cache from pre-computed data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building word cache: 100%|██████████| 83918/83918 [00:00<00:00, 1041489.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF scores built for 83,918 words\n",
      "\n",
      "TF-IDF Statistics:\n",
      "├── Total words: 83,918\n",
      "├── GloVe coverage: 54,322\n",
      "└── Missing words: 29,596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def setup_embeddings_and_tfidf(config: Config, train_texts: List[str], train_labels: List[int]):\n",
    "    \"\"\"Setup GloVe embeddings and TF-IDF scores\"\"\"\n",
    "    \n",
    "    # Load GloVe embeddings\n",
    "    print(\"Loading GloVe embeddings...\")\n",
    "    glove_loader = GloVeLoader(config.glove_dir, config.glove_dim)\n",
    "    embeddings_index = glove_loader.load_embeddings()\n",
    "    \n",
    "    # Build TF-IDF scores from scratch (no cache)\n",
    "    print(\"Building TF-IDF scores...\")\n",
    "    tfidf_builder = SimpleTFIDFBuilder(config)\n",
    "    word_score_cache = tfidf_builder.build_tfidf_scores(train_texts, train_labels)\n",
    "    \n",
    "    return embeddings_index, word_score_cache\n",
    "\n",
    "# Execute embeddings and TF-IDF setup\n",
    "embeddings_index, word_score_cache = setup_embeddings_and_tfidf(\n",
    "    config, train_df[\"text\"].tolist(), train_df[\"label\"].tolist()\n",
    ")\n",
    "\n",
    "print(f\"\\nTF-IDF Statistics:\")\n",
    "print(f\"├── Total words: {len(word_score_cache):,}\")\n",
    "print(f\"├── GloVe coverage: {len([w for w in word_score_cache.keys() if w in embeddings_index]):,}\")\n",
    "print(f\"└── Missing words: {len([w for w in word_score_cache.keys() if w not in embeddings_index]):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09eedce2",
   "metadata": {},
   "source": [
    "## 6. Feature Vektörizasyonu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840c5d3c",
   "metadata": {},
   "source": [
    "### 6.1 Feature Vektör Oluşturma Fonksiyonu\n",
    "\n",
    "Bu hücre train ve test metinleri için hibrit feature vektörlerini oluşturan ana fonksiyonu tanımlar. FeatureVectorizer'ı kullanarak GloVe + TF-IDF hibrit vektörleri oluşturur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67a825bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating feature vectors...\n",
      "TF-IDF KNN Debug Info:\n",
      "  Category 1: 13 words, avg_score=0.0136\n",
      "    Top 3: [('said', '0.0204'), ('iraq', '0.0194'), ('reuters', '0.0184')]\n",
      "  Category 2: 10 words, avg_score=0.0126\n",
      "    Top 3: [('new', '0.0150'), ('game', '0.0145'), ('win', '0.0132')]\n",
      "  Category 3: 18 words, avg_score=0.0140\n",
      "    Top 3: [('oil', '0.0231'), ('reuters', '0.0214'), ('new', '0.0199')]\n",
      "  Category 4: 9 words, avg_score=0.0139\n",
      "    Top 3: [('new', '0.0217'), ('microsoft', '0.0192'), ('software', '0.0142')]\n",
      "Converting texts to vectors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vectorizing texts: 100%|██████████| 120000/120000 [04:31<00:00, 442.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting texts to vectors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vectorizing texts: 100%|██████████| 7600/7600 [00:13<00:00, 553.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature vectors created: (120000, 1200)\n",
      "\n",
      "Vector Dimensions:\n",
      "├── Training vectors: (120000, 1200)\n",
      "├── Test vectors: (7600, 1200)\n",
      "└── Vector type: Hybrid (1200D)\n",
      "Hybrid vectors: 4×300D category-weighted = 1200D total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def create_feature_vectors(config: Config, embeddings_index: Dict, word_score_cache: Dict,\n",
    "                          train_texts: List[str], train_labels: List[int],\n",
    "                          test_texts: List[str], test_labels: List[int]):\n",
    "    \"\"\"Create feature vectors for training and testing\"\"\"\n",
    "    \n",
    "    print(\"Creating feature vectors...\")\n",
    "    \n",
    "    vectorizer = FeatureVectorizer(embeddings_index, word_score_cache, config)\n",
    "    \n",
    "    # TF-IDF KNN debug info\n",
    "    print(\"TF-IDF KNN Debug Info:\")\n",
    "    for category in config.categories:\n",
    "        stats = vectorizer.missing_word_handler.get_category_statistics(category)\n",
    "        print(f\"  Category {category}: {stats['total_words']} words, avg_score={stats['avg_score']:.4f}\")\n",
    "        if stats['top_5']:\n",
    "            print(f\"    Top 3: {[(w, f'{s:.4f}') for w, s in stats['top_5'][:3]]}\")\n",
    "    \n",
    "    # Create full-dimensional vectors\n",
    "    X_train_full = vectorizer.texts_to_vectors(train_texts, train_labels)\n",
    "    X_test_full = vectorizer.texts_to_vectors(test_texts, test_labels)\n",
    "    \n",
    "    print(f\"Feature vectors created: {X_train_full.shape}\")\n",
    "    \n",
    "    return vectorizer, X_train_full, X_test_full\n",
    "\n",
    "# Execute feature vectorization\n",
    "vectorizer, X_train_full, X_test_full = create_feature_vectors(\n",
    "    config, embeddings_index, word_score_cache,\n",
    "    train_df[\"text\"].tolist(), train_df[\"label\"].tolist(),\n",
    "    test_df[\"text\"].tolist(), test_df[\"label\"].tolist()\n",
    ")\n",
    "\n",
    "print(f\"\\nVector Dimensions:\")\n",
    "print(f\"├── Training vectors: {X_train_full.shape}\")\n",
    "print(f\"├── Test vectors: {X_test_full.shape}\")\n",
    "print(f\"└── Vector type: {'Hybrid' if config.use_hybrid_vectors else 'Standard'} ({config.hybrid_vector_dim}D)\")\n",
    "\n",
    "if config.use_hybrid_vectors:\n",
    "    print(f\"Hybrid vectors: {len(config.categories)}×{config.glove_dim}D category-weighted = {config.hybrid_vector_dim}D total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc0fd5a",
   "metadata": {},
   "source": [
    "## 7. Model Eğitimi ve Kaydetme\n",
    "\n",
    "Bu bölümde modeller sırasıyla eğitilip kaydedilecek:\n",
    "1. **LogisticRegression** (Full-dimensional vectors) → Eğit ve Kaydet\n",
    "2. **PCA** (Boyut indirgeme) → Uygula\n",
    "3. **SVM** (PCA vectors) → Eğit ve Kaydet  \n",
    "4. **MLP** (PCA vectors) → Eğit ve Kaydet\n",
    "5. **GradientBoosting** (PCA vectors) → Eğit ve Kaydet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f42c1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training starting...\n",
      "Training Order: LogisticRegression(full-dim) → PCA → SVM/MLP/GradientBoosting(PCA)\n",
      "Strategy: Fast (predefined params)\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "def save_model(model, model_name: str, config: Config, results: Dict):\n",
    "    \"\"\"Model ve sonuçlarını kaydet\"\"\"\n",
    "    os.makedirs(config.models_dir, exist_ok=True)\n",
    "    \n",
    "    # Model dosyasını kaydet\n",
    "    model_path = os.path.join(config.models_dir, f\"{model_name.lower()}_model.pkl\")\n",
    "    joblib.dump(model, model_path)\n",
    "    \n",
    "    # Sonuçları kaydet\n",
    "    results_path = os.path.join(config.models_dir, f\"{model_name.lower()}_results.pkl\")\n",
    "    joblib.dump(results, results_path)\n",
    "    \n",
    "    print(f\"Model saved: {model_path}\")\n",
    "    print(f\"Results saved: {results_path}\")\n",
    "    \n",
    "    return model_path, results_path\n",
    "\n",
    "# Initialize training variables\n",
    "results = {}\n",
    "y_train = train_df[\"label\"].values\n",
    "y_test = test_df[\"label\"].values\n",
    "\n",
    "print(\"Model training starting...\")\n",
    "print(\"Training Order: LogisticRegression(full-dim) → PCA → SVM/MLP/GradientBoosting(PCA)\")\n",
    "print(f\"Strategy: {'Fast (predefined params)' if config.use_predefined_params else 'Search (RandomizedSearchCV)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668f6b32",
   "metadata": {},
   "source": [
    "## 7. Model Eğitimi\n",
    "\n",
    "### 7.1 LogisticRegression Eğitimi (Full-dimensional vectors)\n",
    "\n",
    "Bu hücre LogisticRegression modelini tam boyutlu hibrit vektörlerle (1200D) eğitir ve kaydeder. İlk model olarak eğitilir çünkü yüksek boyutlu vektörlerle iyi çalışır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a669fd51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/4] Training LogisticRegression with full-dimensional vectors...\n",
      "Using predefined optimal params for LogisticRegressionModel...\n",
      "Predefined params applied in 395.13s\n",
      "Best CV Score: 0.9116\n",
      "Test Accuracy: 0.9150\n",
      "Training Time: 395.32s\n",
      "Best Params (predefined): {'C': 100, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "Model saved: models\\logisticregression_model.pkl\n",
      "Results saved: models\\logisticregression_results.pkl\n",
      "✅ LogisticRegression training and saving completed!\n"
     ]
    }
   ],
   "source": [
    "# LogisticRegression Training (Full-dimensional vectors)\n",
    "print(\"\\n[1/4] Training LogisticRegression with full-dimensional vectors...\")\n",
    "lr_model = LogisticRegressionModel(config)\n",
    "start_time = time.time()\n",
    "\n",
    "lr_results = lr_model.hyperparameter_search(X_train_full, y_train)\n",
    "y_pred_lr = lr_model.model.predict(X_test_full)\n",
    "test_acc_lr = accuracy_score(y_test, y_pred_lr)\n",
    "train_time_lr = time.time() - start_time\n",
    "\n",
    "results['LogisticRegression'] = {\n",
    "    'test_accuracy': test_acc_lr,\n",
    "    'cv_score': lr_results['best_score'],\n",
    "    'params': lr_results['best_params'],\n",
    "    'method': lr_results['method'],\n",
    "    'training_time': train_time_lr,\n",
    "    'vector_type': 'full-dimensional',\n",
    "    'predictions': y_pred_lr\n",
    "}\n",
    "\n",
    "print(f\"Best CV Score: {lr_results['best_score']:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc_lr:.4f}\")\n",
    "print(f\"Training Time: {train_time_lr:.2f}s\")\n",
    "print(f\"Best Params ({lr_results['method']}): {lr_results['best_params']}\")\n",
    "\n",
    "# Save LogisticRegression model\n",
    "save_model(lr_model.model, 'LogisticRegression', config, results['LogisticRegression'])\n",
    "print(\"✅ LogisticRegression training and saving completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa710f9",
   "metadata": {},
   "source": [
    "### 7.2 PCA Boyut İndirgeme\n",
    "\n",
    "Bu hücre hibrit vektörleri (1200D) PCA ile boyut indirger (100D). Sonraki modeller bu indirgenmiş vektörleri kullanacak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ed14399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2/4] Applying PCA dimensionality reduction...\n",
      "PCA fitting: 1200D → 100D\n",
      "Input: Hybrid vectors (4×300D TF-IDF weighted)\n",
      "PCA fitted, explained variance ratio: 0.9895\n",
      "PCA transformation: 1200D → 100D\n",
      "Explained variance ratio: 0.9895\n",
      "PCA transformer saved: models\\pca_transformer.pkl\n",
      "✅ PCA dimensionality reduction completed!\n"
     ]
    }
   ],
   "source": [
    "# PCA Dimensionality Reduction\n",
    "print(\"\\n[2/4] Applying PCA dimensionality reduction...\")\n",
    "vectorizer.fit_pca(X_train_full)\n",
    "X_train_pca = vectorizer.transform_pca(X_train_full)\n",
    "X_test_pca = vectorizer.transform_pca(X_test_full)\n",
    "\n",
    "print(f\"PCA transformation: {X_train_full.shape[1]}D → {X_train_pca.shape[1]}D\")\n",
    "print(f\"Explained variance ratio: {vectorizer.pca.explained_variance_ratio_.sum():.4f}\")\n",
    "\n",
    "# Save PCA transformer\n",
    "pca_path = os.path.join(config.models_dir, \"pca_transformer.pkl\")\n",
    "joblib.dump(vectorizer.pca, pca_path)\n",
    "print(f\"PCA transformer saved: {pca_path}\")\n",
    "print(\"✅ PCA dimensionality reduction completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e02ff3",
   "metadata": {},
   "source": [
    "### 7.3 SVM Eğitimi (PCA vectors)\n",
    "\n",
    "Bu hücre Support Vector Machine modelini PCA ile indirgenmiş vektörlerle (100D) eğitir ve kaydeder. RBF kernel ile nonlinear sınıflandırma yapar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae19a960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3/5] Training SVM with PCA vectors...\n",
      "Using predefined optimal params for SVMModel...\n",
      "Predefined params applied in 636.12s\n",
      "Best CV Score: 0.9097\n",
      "Test Accuracy: 0.9099\n",
      "Training Time: 650.27s\n",
      "Best Params (predefined): {'C': 100, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "Model saved: models\\svm_model.pkl\n",
      "Results saved: models\\svm_results.pkl\n",
      "✅ SVM training and saving completed!\n"
     ]
    }
   ],
   "source": [
    "# SVM Training (PCA-reduced vectors)\n",
    "print(\"\\n[3/5] Training SVM with PCA vectors...\")\n",
    "svm_model = SVMModel(config)\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Hyperparameter search on PCA vectors\n",
    "    search_results = svm_model.hyperparameter_search(X_train_pca, y_train)\n",
    "    \n",
    "    # Test prediction on PCA vectors\n",
    "    y_pred = svm_model.model.predict(X_test_pca)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    results['SVM'] = {\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'cv_score': search_results['best_score'],\n",
    "        'params': search_results['best_params'],\n",
    "        'method': search_results['method'],\n",
    "        'training_time': training_time,\n",
    "        'vector_type': 'PCA-reduced',\n",
    "        'predictions': y_pred\n",
    "    }\n",
    "    \n",
    "    print(f\"Best CV Score: {search_results['best_score']:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Training Time: {training_time:.2f}s\")\n",
    "    print(f\"Best Params ({search_results['method']}): {search_results['best_params']}\")\n",
    "    \n",
    "    # Save SVM model\n",
    "    save_model(svm_model.model, 'SVM', config, results['SVM'])\n",
    "    print(\"✅ SVM training and saving completed!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error training SVM: {e}\")\n",
    "    results['SVM'] = {'error': str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2757de",
   "metadata": {},
   "source": [
    "### 7.4 MLP Eğitimi (PCA vectors)\n",
    "\n",
    "Bu hücre Multi-Layer Perceptron (Neural Network) modelini PCA ile indirgenmiş vektörlerle (100D) eğitir ve kaydeder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2702039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[4/5] Training MLP with PCA vectors...\n",
      "Using predefined optimal params for MLPModel...\n",
      "Predefined params applied in 11625.65s\n",
      "Best CV Score: 0.9189\n",
      "Test Accuracy: 0.9272\n",
      "Training Time: 11626.33s\n",
      "Best Params (predefined): {'hidden_layer_sizes': (200, 100), 'activation': 'relu', 'alpha': 0.0001, 'learning_rate': 'constant'}\n",
      "Model saved: models\\mlp_model.pkl\n",
      "Results saved: models\\mlp_results.pkl\n",
      "✅ MLP training and saving completed!\n"
     ]
    }
   ],
   "source": [
    "# MLP Training (PCA-reduced vectors)\n",
    "print(\"\\n[4/5] Training MLP with PCA vectors...\")\n",
    "mlp_model = MLPModel(config)\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Hyperparameter search on PCA vectors\n",
    "    search_results = mlp_model.hyperparameter_search(X_train_pca, y_train)\n",
    "    \n",
    "    # Test prediction on PCA vectors\n",
    "    y_pred = mlp_model.model.predict(X_test_pca)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    results['MLP'] = {\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'cv_score': search_results['best_score'],\n",
    "        'params': search_results['best_params'],\n",
    "        'method': search_results['method'],\n",
    "        'training_time': training_time,\n",
    "        'vector_type': 'PCA-reduced',\n",
    "        'predictions': y_pred\n",
    "    }\n",
    "    \n",
    "    print(f\"Best CV Score: {search_results['best_score']:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Training Time: {training_time:.2f}s\")\n",
    "    print(f\"Best Params ({search_results['method']}): {search_results['best_params']}\")\n",
    "    \n",
    "    # Save MLP model\n",
    "    save_model(mlp_model.model, 'MLP', config, results['MLP'])\n",
    "    print(\"✅ MLP training and saving completed!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error training MLP: {e}\")\n",
    "    results['MLP'] = {'error': str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a129c58",
   "metadata": {},
   "source": [
    "### 7.5 GradientBoosting Eğitimi (PCA vectors)\n",
    "\n",
    "Bu hücre Gradient Boosting Classifier modelini PCA ile indirgenmiş vektörlerle (100D) eğitir ve kaydeder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11000085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[5/5] Training GradientBoosting with PCA vectors...\n",
      "Using predefined optimal params for GradientBoostingModel...\n",
      "Predefined params applied in 9534.25s\n",
      "Best CV Score: 0.9130\n",
      "Test Accuracy: 0.9143\n",
      "Training Time: 9534.38s\n",
      "Best Params (predefined): {'n_estimators': 200, 'learning_rate': 0.2, 'max_depth': 5, 'subsample': 0.9}\n",
      "Model saved: models\\gradientboosting_model.pkl\n",
      "Results saved: models\\gradientboosting_results.pkl\n",
      "✅ GradientBoosting training and saving completed!\n",
      "\n",
      "🎉 All models training completed!\n"
     ]
    }
   ],
   "source": [
    "# GradientBoosting Training (PCA-reduced vectors)\n",
    "print(\"\\n[5/5] Training GradientBoosting with PCA vectors...\")\n",
    "gb_model = GradientBoostingModel(config)\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Hyperparameter search on PCA vectors\n",
    "    search_results = gb_model.hyperparameter_search(X_train_pca, y_train)\n",
    "    \n",
    "    # Test prediction on PCA vectors\n",
    "    y_pred = gb_model.model.predict(X_test_pca)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    results['GradientBoosting'] = {\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'cv_score': search_results['best_score'],\n",
    "        'params': search_results['best_params'],\n",
    "        'method': search_results['method'],\n",
    "        'training_time': training_time,\n",
    "        'vector_type': 'PCA-reduced',\n",
    "        'predictions': y_pred\n",
    "    }\n",
    "    \n",
    "    print(f\"Best CV Score: {search_results['best_score']:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Training Time: {training_time:.2f}s\")\n",
    "    print(f\"Best Params ({search_results['method']}): {search_results['best_params']}\")\n",
    "    \n",
    "    # Save GradientBoosting model\n",
    "    save_model(gb_model.model, 'GradientBoosting', config, results['GradientBoosting'])\n",
    "    print(\"✅ GradientBoosting training and saving completed!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error training GradientBoosting: {e}\")\n",
    "    results['GradientBoosting'] = {'error': str(e)}\n",
    "\n",
    "print(\"\\n🎉 All models training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68e400a",
   "metadata": {},
   "source": [
    "## 8. Sonuç Analizi ve Karşılaştırma\n",
    "\n",
    "Bu bölümde tüm modellerin performansları karşılaştırılır ve en iyi model belirlenir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04f00a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL RESULTS SUMMARY\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Vector Type</th>\n",
       "      <th>Method</th>\n",
       "      <th>CV Score</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Training Time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>full-dimensional</td>\n",
       "      <td>predefined</td>\n",
       "      <td>0.9116</td>\n",
       "      <td>0.9150</td>\n",
       "      <td>395.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVM</td>\n",
       "      <td>PCA-reduced</td>\n",
       "      <td>predefined</td>\n",
       "      <td>0.9097</td>\n",
       "      <td>0.9099</td>\n",
       "      <td>650.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLP</td>\n",
       "      <td>PCA-reduced</td>\n",
       "      <td>predefined</td>\n",
       "      <td>0.9189</td>\n",
       "      <td>0.9272</td>\n",
       "      <td>11626.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>PCA-reduced</td>\n",
       "      <td>predefined</td>\n",
       "      <td>0.9130</td>\n",
       "      <td>0.9143</td>\n",
       "      <td>9534.38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Model       Vector Type      Method CV Score Test Accuracy  \\\n",
       "0  LogisticRegression  full-dimensional  predefined   0.9116        0.9150   \n",
       "1                 SVM       PCA-reduced  predefined   0.9097        0.9099   \n",
       "2                 MLP       PCA-reduced  predefined   0.9189        0.9272   \n",
       "3    GradientBoosting       PCA-reduced  predefined   0.9130        0.9143   \n",
       "\n",
       "  Training Time (s)  \n",
       "0            395.32  \n",
       "1            650.27  \n",
       "2          11626.33  \n",
       "3           9534.38  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Model: MLP\n",
      "Test Accuracy: 0.9272\n",
      "CV Score: 0.9189\n",
      "\n",
      "Classification Report - MLP:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       World       0.92      0.94      0.93      1900\n",
      "      Sports       0.94      0.95      0.95      1900\n",
      "    Business       0.94      0.90      0.92      1900\n",
      "    Sci/Tech       0.90      0.92      0.91      1900\n",
      "\n",
      "    accuracy                           0.93      7600\n",
      "   macro avg       0.93      0.93      0.93      7600\n",
      "weighted avg       0.93      0.93      0.93      7600\n",
      "\n",
      "\n",
      "Confusion Matrix - MLP:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>World</th>\n",
       "      <th>Sports</th>\n",
       "      <th>Business</th>\n",
       "      <th>Sci/Tech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>World</th>\n",
       "      <td>1780</td>\n",
       "      <td>41</td>\n",
       "      <td>26</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sports</th>\n",
       "      <td>38</td>\n",
       "      <td>1810</td>\n",
       "      <td>17</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Business</th>\n",
       "      <td>60</td>\n",
       "      <td>29</td>\n",
       "      <td>1711</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sci/Tech</th>\n",
       "      <td>49</td>\n",
       "      <td>38</td>\n",
       "      <td>67</td>\n",
       "      <td>1746</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          World  Sports  Business  Sci/Tech\n",
       "World      1780      41        26        53\n",
       "Sports       38    1810        17        35\n",
       "Business     60      29      1711       100\n",
       "Sci/Tech     49      38        67      1746"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pipeline completed successfully!\n",
      "Best performing model: MLP (0.9272 accuracy)\n",
      "Training completed at: 2025-07-08 09:45:16\n"
     ]
    }
   ],
   "source": [
    "def analyze_results(results: Dict, y_test: np.ndarray):\n",
    "    \"\"\"Analyze and display results\"\"\"\n",
    "    results_data = []\n",
    "    for model_name, result in results.items():\n",
    "        if 'error' not in result:\n",
    "            results_data.append({\n",
    "                'Model': model_name,\n",
    "                'Vector Type': result['vector_type'],\n",
    "                'Method': result['method'],\n",
    "                'CV Score': f\"{result['cv_score']:.4f}\",\n",
    "                'Test Accuracy': f\"{result['test_accuracy']:.4f}\",\n",
    "                'Training Time (s)': f\"{result['training_time']:.2f}\"\n",
    "            })\n",
    "    \n",
    "    df_results = pd.DataFrame(results_data)\n",
    "    print(\"MODEL RESULTS SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    display(df_results)\n",
    "    \n",
    "    # Find best model\n",
    "    best_model_name = max(\n",
    "        [name for name, result in results.items() if 'error' not in result], \n",
    "        key=lambda x: results[x]['test_accuracy']\n",
    "    )\n",
    "    best_result = results[best_model_name]\n",
    "    \n",
    "    print(f\"\\nBest Model: {best_model_name}\")\n",
    "    print(f\"Test Accuracy: {best_result['test_accuracy']:.4f}\")\n",
    "    print(f\"CV Score: {best_result['cv_score']:.4f}\")\n",
    "    \n",
    "    # Classification report for best model\n",
    "    y_pred_best = best_result['predictions']\n",
    "    target_names = ['World', 'Sports', 'Business', 'Sci/Tech']\n",
    "    report = classification_report(y_test, y_pred_best, target_names=target_names)\n",
    "    print(f\"\\nClassification Report - {best_model_name}:\")\n",
    "    print(report)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred_best)\n",
    "    cm_df = pd.DataFrame(cm, index=target_names, columns=target_names)\n",
    "    print(f\"\\nConfusion Matrix - {best_model_name}:\")\n",
    "    display(cm_df)\n",
    "    \n",
    "    return df_results, best_model_name, best_result\n",
    "\n",
    "# Execute analysis\n",
    "results_df, best_model, best_result = analyze_results(results, y_test)\n",
    "\n",
    "print(f\"\\nPipeline completed successfully!\")\n",
    "print(f\"Best performing model: {best_model} ({best_result['test_accuracy']:.4f} accuracy)\")\n",
    "print(f\"Training completed at: {time.strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3873c6e3",
   "metadata": {},
   "source": [
    "## 9. Essential Components Kaydetme\n",
    "\n",
    "Bu bölümde gelecekte prediction yapmak için gerekli tüm bileşenler kaydedilir:\n",
    "- **FeatureVectorizer**: Metin → hibrit vektör dönüşümü\n",
    "- **GloVe embeddings**: Word embeddings \n",
    "- **TF-IDF scores**: Kategori-bazlı ağırlık hesaplamaları\n",
    "- **Training config**: Aynı parametrelerle çalışma\n",
    "(sonradan eklendi vectorizerları unutmuşum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0a8c6d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving essential components for future predictions...\n",
      "FeatureVectorizer saved: models\\feature_vectorizer.pkl\n",
      "Training config saved: models\\training_config.pkl\n",
      "\n",
      "All essential components saved!\n",
      "\n",
      "Saved files in models directory:\n",
      "├── feature_vectorizer.pkl      (Text → Vector conversion)\n",
      "├── pca_transformer.pkl         (PCA for dimensionality reduction)\n",
      "├── training_config.pkl         (Training configuration)\n",
      "└── [model_name]_model.pkl      (Trained models)\n",
      "\n",
      "To make predictions on new text:\n",
      "1. Load feature_vectorizer.pkl\n",
      "2. Load the best model from results\n",
      "3. Use vectorizer.text_to_vector() → PCA → model.predict()\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSaving essential components for future predictions...\")\n",
    "\n",
    "# 1. FeatureVectorizer'ı kaydet (en önemli!)\n",
    "vectorizer_path = os.path.join(config.models_dir, \"feature_vectorizer.pkl\")\n",
    "joblib.dump(vectorizer, vectorizer_path)\n",
    "print(f\"FeatureVectorizer saved: {vectorizer_path}\")\n",
    "\n",
    "# 4. Config'i kaydet\n",
    "config_path = os.path.join(config.models_dir, \"training_config.pkl\")\n",
    "joblib.dump(config, config_path)\n",
    "print(f\"Training config saved: {config_path}\")\n",
    "\n",
    "print(\"\\nAll essential components saved!\")\n",
    "print(\"\\nSaved files in models directory:\")\n",
    "print(\"├── feature_vectorizer.pkl      (Text → Vector conversion)\")\n",
    "print(\"├── pca_transformer.pkl         (PCA for dimensionality reduction)\")\n",
    "print(\"├── training_config.pkl         (Training configuration)\")\n",
    "print(\"└── [model_name]_model.pkl      (Trained models)\")\n",
    "\n",
    "print(f\"\\nTo make predictions on new text:\")\n",
    "print(\"1. Load feature_vectorizer.pkl\")\n",
    "print(\"2. Load the best model from results\")\n",
    "print(\"3. Use vectorizer.text_to_vector() → PCA → model.predict()\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
